<!DOCTYPE html>
<html lang="en" dir="auto">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="noindex, nofollow">
<title>Entropy Collapse and Mitigation Strategies | Qiangwei Bai&#39;s Blog</title>
<meta name="keywords" content="">
<meta name="description" content="1. Policy Entropy and Entropy Collapse
1.1 Entropy Definition
​	Let $x$ denote the prompt and $y$ denote the response. The policy $\pi_{\theta}$ outputs a probability distribution for a token $t$ as follows:
$$
p_t=(p_{t,1},\dots,p_{t,|V|})=\pi_{\theta}(\cdot|x,y_{\lt t})=\text{softmax}(\frac{z_t}{T}) \quad (1)
$$
Here, $|V|$ is the size of the vocabulary, $z_t\in\mathbb{R}^V$ arg the logits, and $T\in\mathbb{R}$ is the decoding temprature.
​	The entropy for token $t$ is then given by:
$$
H_t=-\sum_{j=1}^{|V|} p_{t,j}\log p_{t,j} \quad (2)
$$">
<meta name="author" content="Qiangwei Bai">
<link rel="canonical" href="http://localhost:1313/posts/entropy-collapse-and-mitigation-strategies/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.476a77ed88e359f5e3ff93b72a898819adeee2a2e06fe281e5d88eea72a52038.css" integrity="sha256-R2p37YjjWfXj/5O3KomIGa3u4qLgb&#43;KB5diO6nKlIDg=" rel="preload stylesheet" as="style">
<link rel="icon" href="http://localhost:1313/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="http://localhost:1313/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://localhost:1313/favicon-32x32.png">
<link rel="apple-touch-icon" href="http://localhost:1313/apple-touch-icon.png">
<link rel="mask-icon" href="http://localhost:1313/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="http://localhost:1313/posts/entropy-collapse-and-mitigation-strategies/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css" integrity="sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js" integrity="sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js" integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous"></script>
<script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {
            
            delimiters: [
                {left: '$$', right: '$$', display: true},
                {left: '$', right: '$', display: false}
            ],
            
            throwOnError: false,
            errorColor: '#cc0000',
            trust: (context) => ['\\htmlId', '\\href'].includes(context.command),
            macros: {
                "\\eqref": "\\href{###1}{(\\text{#1})}",
                "\\ref": "\\href{###1}{\\text{#1}}",
                "\\label": "\\htmlId{#1}{}"
            }
        });
    });
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="http://localhost:1313/" accesskey="h" title="Qiangwei Bai&#39;s Blog (Alt + H)">Qiangwei Bai&#39;s Blog</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="http://localhost:1313/archives" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/search/" title="Search">
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/categories/" title="Categories">
                    <span>Categories</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="http://localhost:1313/">Home</a>&nbsp;»&nbsp;<a href="http://localhost:1313/posts/">Posts</a></div>
    <h1 class="post-title entry-hint-parent">
      Entropy Collapse and Mitigation Strategies
    </h1>
    <div class="post-meta"><span title='2025-07-05 18:49:41 +0800 CST'>July 5, 2025</span>&nbsp;·&nbsp;9 min&nbsp;·&nbsp;Qiangwei Bai

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#1-policy-entropy-and-entropy-collapse" aria-label="1. Policy Entropy and Entropy Collapse">1. Policy Entropy and Entropy Collapse</a><ul>
                        
                <li>
                    <a href="#11-entropy-definition" aria-label="1.1 Entropy Definition">1.1 Entropy Definition</a></li>
                <li>
                    <a href="#12-entropy-collapse-and-model-performance" aria-label="1.2 Entropy Collapse and Model Performance">1.2 Entropy Collapse and Model Performance</a></li>
                <li>
                    <a href="#13-token-entropy-and-forking-tokens" aria-label="1.3 Token Entropy and Forking Tokens">1.3 Token Entropy and Forking Tokens</a></li></ul>
                </li>
                <li>
                    <a href="#2-grpo" aria-label="2. GRPO">2. GRPO</a></li>
                <li>
                    <a href="#3-direct-optimization-for-forking-tokens" aria-label="3. Direct Optimization for Forking Tokens">3. Direct Optimization for Forking Tokens</a></li>
                <li>
                    <a href="#4-the-impact-of-positive-and-negative-samples" aria-label="4. The Impact of Positive and Negative Samples">4. The Impact of Positive and Negative Samples</a></li>
                <li>
                    <a href="#5-the-change-in-entropy" aria-label="5. The Change in Entropy">5. The Change in Entropy</a></li>
                <li>
                    <a href="#6-a-unified-perspective" aria-label="6. A Unified Perspective">6. A Unified Perspective</a></li>
                <li>
                    <a href="#references" aria-label="References">References</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><h1 id="1-policy-entropy-and-entropy-collapse">1. Policy Entropy and Entropy Collapse<a hidden class="anchor" aria-hidden="true" href="#1-policy-entropy-and-entropy-collapse">#</a></h1>
<h2 id="11-entropy-definition">1.1 Entropy Definition<a hidden class="anchor" aria-hidden="true" href="#11-entropy-definition">#</a></h2>
<p>​	Let $x$ denote the prompt and $y$ denote the response. The policy $\pi_{\theta}$ outputs a probability distribution for a token $t$ as follows:
$$
p_t=(p_{t,1},\dots,p_{t,|V|})=\pi_{\theta}(\cdot|x,y_{\lt t})=\text{softmax}(\frac{z_t}{T}) \quad (1)
$$</p>
<p>Here, $|V|$ is the size of the vocabulary, $z_t\in\mathbb{R}^V$ arg the <code>logits</code>, and $T\in\mathbb{R}$ is the decoding temprature.</p>
<p>​	The entropy for token $t$ is then given by:
$$
H_t=-\sum_{j=1}^{|V|} p_{t,j}\log p_{t,j} \quad (2)
$$</p>
<h2 id="12-entropy-collapse-and-model-performance">1.2 Entropy Collapse and Model Performance<a hidden class="anchor" aria-hidden="true" href="#12-entropy-collapse-and-model-performance">#</a></h2>
<p>​	In the early stages of RL training, the model&rsquo;s entropy drops sharply. As entropy decreases, accuracy enters a period of rapid growth. However, the rapid depletion of entropy can lead to the model becoming overconfident, which in turn diminishes its exploration capabilities. Through empirical studies, [1] established a quantitative relationship between policy entropy $H$ and downstream task performance $R$:
$$
R=-a\cdot\exp(H)+b \quad (3)
$$
where $a$ and $b$ are fitting coefficients that reflect the intrinsic properties of the specific model and training data.</p>
<p>​	<strong>Evidently, maintaining entropy within a reasonable range over the longer timeframe is crucial for the continuous improvement of the model&rsquo;s capabilities.</strong> By stabilizing entropy to enable longer RL training, [2] found that model can push past its original performance limits and achieve continuous improvement.</p>
<h2 id="13-token-entropy-and-forking-tokens">1.3 Token Entropy and Forking Tokens<a hidden class="anchor" aria-hidden="true" href="#13-token-entropy-and-forking-tokens">#</a></h2>
<p>​	Analysis  by [3] reveals that while most token have very low entropy, a small number of token exhibit high entropy. Furthermore, the function of a token is highly correlated with it entropy:</p>
<ul>
<li><strong>High-entropy tokens</strong> primarily serve as &ldquo;logic connector&rdquo; and &ldquo;hypothesis introducer&rdquo;, such as <code>wait</code>,<code>however</code>, etc.</li>
<li><strong>Low-entropy tokens</strong>, on the other hand, act as &ldquo;structure completer&rdquo;, responsible for filling in details within well-established reasoning steps.</li>
</ul>
<p>​	Consequently, [3] defines these high-entropy tokens as <code>forking tokens</code>.</p>
<hr>
<h1 id="2-grpo">2. GRPO<a hidden class="anchor" aria-hidden="true" href="#2-grpo">#</a></h1>
<p>​	GRPO builds upon PPO by computing advantages through intra-group normalization. Specifically, give a prompt $x$, we sample $G$ response ${y_i}<em>{i=1}^G$. The advantages is then calculated as:
$$
A</em>{i,t}=\frac{r_i-\text{mean}({r_i}<em>{i=1}^G)}{\text{std}({r_i}</em>{i=1}^G)} \quad (4)
$$
where $r_i$ is reward value for the response $y_i$.</p>
<p>​	The objective function of GRPO is:
$$
J_{\text{GRPO}}=\mathbb{E}<em>{x\sim p,{y_i}</em>{i=1}^G\sim\pi_{\text{old}}(\cdot|x)}\left[
\frac{1}{G}\sum_{i=1}^G\frac{1}{|y_i|}\sum_{t=1}^{|y_i|}\min\left(r_{i,t}(\theta)A_{i,t},\text{clip}(r_{i,t}(\theta),1-\varepsilon,1+\varepsilon)A_{i,t}\right)
\right] \quad (5)
$$
where $r_{i,t}(\theta)=\frac{\pi_{\theta}(a_{i,t}|s_{i,t})}{\pi_{\theta_{\text{old}}}(a_{i,t}|s_{i,t})}$ is the importance sampling ratio.</p>
<hr>
<h1 id="3-direct-optimization-for-forking-tokens">3. Direct Optimization for Forking Tokens<a hidden class="anchor" aria-hidden="true" href="#3-direct-optimization-for-forking-tokens">#</a></h1>
<p>​	<strong>Decoupling the $\text{clip}$ Upper and Lower Bounds</strong>. [4] argues that the $\text{clip}$ operation is inequitable for high-probability(low-entropy) tokens and low-probability(high-entropy) tokens. For examples, assume $\varepsilon=0.2$. For a token with a probability of $\pi_{\theta_{\text{old}}}=0.9$, the $\text{clip}$ operation limits its update, allowing for a maximum absolute probability increase of $1.08-0.9=0.18$. In contrast, for a token with a probability $\pi_{\theta_{\text{old}}}=0.01$, the maximum absolute increase is merely $0.01\times 1.2=0.012$. Therefore, the $\text{clip}$ operation restricts the update magnitude for low-probability(high-entropy) tokens. To address this, [4] proposes decoupling the clipping bounds from a single $\varepsilon$ into $\varepsilon_{low}$ and $\varepsilon_{high}$, and appropriately increasing $\varepsilon_{high}$ to allow high-entropy tokens to receive larger updates.
$$
J_{\text{DAPO}}=\mathbb{E}<em>{x\sim p,{y_i}</em>{i=1}^G\sim\pi_{\text{old}}(\cdot|x)}\left[
\frac{1}{G}\sum_{i=1}^G\frac{1}{|y_i|}\sum_{t=1}^{|y_i|}\min\left(r_{i,t}(\theta)A_{i,t},\hat{r}<em>{i,t}(\theta)A</em>{i,t}\right)
\right] \quad (6)
$$
where $\hat{r}<em>{i,t}(\theta)=\text{clip}(r</em>{i,t}(\theta),1-\varepsilon_{low},1+\varepsilon_{high})$.</p>
<p>​	<strong>Decoupling the Advantage from Clipping</strong>. Building on [4], [5] further argues that the advantage term should be decoupled from the clipping mechanism. This is because when clipping is triggered, the gradient becomes zero. The importance sampling ratio should instead be viewed as a weight for the advantage. Therefore, the objective function is further modified to:
$$
J_{\text{CISPO}}=\mathbb{E}<em>{x\sim p,{y_i}</em>{i=1}^G\sim\pi_{\text{old}}(\cdot|x)}\left[
\frac{1}{G}\sum_{i=1}^G\frac{1}{|y_i|}\sum_{t=1}^{|y_i|}\text{sg}(\hat{r}<em>{i,t}(\theta))A</em>{i,t}
\right] \quad (7)
$$
where <code>sg</code> is the <code>stop_gradient</code> operation.</p>
<p>​	<strong>Directly Increasing the Advantage of <code>forking tokens</code>.</strong> [6] takes an even more direct approach: since high-entropy tokens are more important, their advantage should be directly increased.
$$
\begin{align*}
\psi(H_{i,t})&amp;=\min\Big(\alpha\cdot H_{i,t}^{\text{detach}},\frac{|A_{i,t}|}{k}\Big) \quad (8) \
\hat{A}<em>{i,t}&amp;=A</em>{i,t}+\psi(H_{i,t}) \quad (9)\
\end{align*} \
$$
Here, $\psi(H_{i,t})$ servers as an advantage bonus, which is capped to ensure it does not exceed $1/k$ of the original advantage&rsquo;s magnitude.</p>
<hr>
<h1 id="4-the-impact-of-positive-and-negative-samples">4. The Impact of Positive and Negative Samples<a hidden class="anchor" aria-hidden="true" href="#4-the-impact-of-positive-and-negative-samples">#</a></h1>
<p>​	[7] analyses the impact of positive and negative samples on entropy collapse and finds in the experiments that surprisingly good results can be achieved using only negative samples. Consequently, for the case of binary rewards(+1 for positive samples, -1 for negative), [7] further analyses the gradient of the REINFORCE loss with respect to the <code>logits</code>.
$$
-\frac{\partial L_t}{\partial z_{t,v}}=\begin{cases}
r\cdot(1-\pi_{v})&amp;v=y_t \
-r\cdot\pi_{v} &amp;v\neq y_t
\end{cases} \quad (10)
$$
where $z_{t,v}$ is the component for token $v$ in the <code>logits</code> vector $z_{t}$.</p>
<div class="proof-container">
    <div class="proof-header">
        <strong>Proof</strong>
    </div>
    <div class="proof-content">
        <p>​	The gradient at a single timestep $t$ is $\nabla L_{t}=-r\nabla\log\pi_{\theta}(y_t)$. Expanding $\log\pi_{\theta}(y_t)$, we get:
$$
\log\pi_{\theta}(y_t)=\log\left(\frac{\exp(z_{t,{y_t}})}{\sum_{v&rsquo;\in V}\exp(z_{t,{v&rsquo;}})}\right)=z_{t,{y_t}}-\log\left(\sum_{v&rsquo;\in V}\exp(z_{t,{v&rsquo;}})\right)
$$
​	We discuss this in two cases:</p>
<p>​	<strong>Case 1</strong>: $v=y_t$, i.e., taking the derivative with respect to logit of the sampling token.
$$
\begin{align*}
\frac{\partial(\log\pi_{\theta}(y_t))}{\partial z_{t,{y_t}}}&amp;=\frac{\partial z_{t,{y_t}}}{\partial z_{t,{y_t}}}-\frac{\partial}{\partial z_{t,{y_t}}}\log\left(\sum_{v&rsquo;\in V}\exp(z_{t,{v&rsquo;}})\right) \
&amp;=1-\frac{1}{\sum_{v&rsquo;}\exp(z_{t,{v&rsquo;}})}\cdot\exp(z_{t,{y_t}}) \
&amp;=1-\pi_{y_t}
\end{align*}
$$
​	<strong>Case 2</strong>: $v\neq y_t$, i.e., taking the derivative with respect to logit of a non-sampled token.
$$
\begin{align*}
\frac{\partial(\log\pi_{\theta}(y_t))}{\partial z_{t,{v}}}&amp;=\frac{\partial z_{t,{y_t}}}{\partial z_{t,{v}}}-\frac{\partial}{\partial z_{t,{v}}}\log\left(\sum_{v&rsquo;\in V}\exp(z_{t,{v&rsquo;}})\right) \
&amp;=0-\frac{1}{\sum_{v&rsquo;}\exp(z_{t,{v&rsquo;}})}\cdot\exp(z_{t,{v}}) \
&amp;=-\pi_{v}
\end{align*}
$$
​	Combing these two cases, the gradient of the loss is:
$$
\frac{\partial L_t}{\partial z_{t,v}}=\begin{cases}
-r\cdot(1-\pi_{y_t})&amp;v=y_t \
r\cdot\pi_v &amp;v\neq y_t
\end{cases} \
$$</p>

    </div>
</div>
<p>​	<strong>When $r=1$(positive reward)</strong>. For the sampled token $y_t$, the gradient increases its logits $z_{t,y_t}$ with a magnitude of $(1-\pi_{y_t})$. When the model is not confident about $y_t$(i.e., $\pi_{y_t}$ is low), it is updated with larger magnitude. For non-sampled tokens, the gradient decreases it logits $z_{t,v}$ with a magnitude of $\pi_v$. From the perspective of forking tokens [3], if a positive sample contains a forking token, that token&rsquo;s entropy will decreases more rapidly, meaning these become determined more quickly.</p>
<p>​	<strong>When $r=-1$(negative reward).</strong> For the sampled token $y_t$, the gradient decreases its logits $z_{t,y_t}$ with a magnitude of $(1-\pi_{y_t})$. Simultaneously, the released probability mass is redistributed among the other tokens in proportion to their own probability.</p>
<p>​	In summary, tokens sampled in positive instances proportionally strip probability mass from the remaining tokens, whereas tokens sampled in negative instances cause the stripped probability mass to be proportionally redistributed among the remaining tokens. Therefore, negative samples are naturally conducive to increasing entropy.</p>
<p>​	Based on this analysis, [7] propose reducing the signal strength of positive samples during training to maintain entropy.</p>
<blockquote>
<p><strong>My thought</strong>: The effectiveness of training with purely negative samples might be because it continually reinforces the base model&rsquo;s original distribution. Could this be analogous to the recent wave of methods based on internal feedback?[8]</p></blockquote>
<hr>
<h1 id="5-the-change-in-entropy">5. The Change in Entropy<a hidden class="anchor" aria-hidden="true" href="#5-the-change-in-entropy">#</a></h1>
<p>​	In contrast to the foregoing methods, [1] shifts the focus from entropy itself to the change in entropy. Viewing entropy as a function of the <code>logits</code>, i.e., $H_t(z_t)$, the change in entropy under a sufficiently small update step can be approximated as:
$$
H_t(z_t^{k+1}) - H_t(z_t^k)\approx-\text{Cov}<em>{y_t\sim\pi(\cdot|z_t^k)}\Big(\log\pi(y_t|z_t^k),\Delta z</em>{t,y_t}^k\Big) \quad (11)
$$
where $z_t^k$ and $z_t^{k+1}$ represent the <code>logits</code> vectors at two consecutive steps, and $\Delta z_{t,y_t}^k=z_{t,y_t}^{k+1}-z_{t,y_t}^{k}$ is the change in the <code>logits</code> of token $y_t$ between these steps.</p>
<p>​	Equation (11) reveals that the change in entropy is negatively correlated with the covariance between the log-probability and the change in <code>logits</code>. Specifically, entropy decreases when the <code>logits</code> of high-probability tokens increase or the <code>logits</code> of low-probability tokens decrease; otherwise, entropy increases. More intuitively,  sharpening the current model&rsquo;s distribution leads to a reduction in entropy.</p>
<div class="proof-container">
    <div class="proof-header">
        <strong>Proof</strong>
    </div>
    <div class="proof-content">
        ​	The definition of entropy can be expressed as:
$$
H_t(z_t)=-\mathbb{E}<em>{y_t\sim\pi(\cdot|z_t)}[\log \pi(y_t|z_t])
$$
​	Let the <code>logits</code> at step $k$ be $z_t^k$, and after a training step, they become $z</em>{t}^{k+1}$ at step $k+1$. Since the learning rate is typically small in practice, we can estimate the change in entropy between $z_t^k$ and $z_t^{k+1}$ using Taylor expansion. Specifically, we perform a first-order Taylor expansion of the function $H_t(z_t)$ at point $z_t^k$:
$$
H_t(z_t)\approx H_t(z_t^k)+\langle \nabla_{z_t}H_t(z_t^k),z_t - z_t^k \rangle
$$
Substituting $z_t^{k+1}$ in the equation, we have:
$$
H_t(z_t^{k+1}) - H_t(z_t^k)\approx\langle \nabla_{z_t}H_t(z_t^k),z_t^{k+1} - z_t^k \rangle
$$
​	First, let&rsquo;s find $\nabla_{z_t^k} H_t(z_t^k)$:
$$
\begin{align*}
\nabla_{z_t^k}H_t(z_t^k) &amp;= \nabla_{z_t^k}(-\mathbb{E}<em>{y_t\sim\pi(\cdot|z_t^k)}[\log \pi(y_t|z_t^k]) \
&amp;=-\sum</em>{y_t}\Big[\nabla_{z_t^k}\pi(y_t|z_t^k)\log\pi(y_t|z_t^k)+\pi(y_t|z_t^k)\nabla_{z_t^k}\log\pi(y_t|z_t^k)\Big] \
&amp;=-\sum_{y_t}\Big[\pi({y_t|z_t^k)}\nabla_{z_t^k}\log\pi(y_t|z_t^k)\cdot \log\pi(y_t|z_t^k)+\pi(y_t|z_t^k)\nabla_{z_t^k}\log\pi(y_t|z_t^k)\Big]\
&amp;=-\mathbb{E}<em>{y_t\sim\pi(\cdot|z_t^k)}\Big[\nabla</em>{z_t^k}\log\pi(y_t|z_t^k)\cdot \log\pi(y_t|z_t^k) + \nabla_{z_t^k}\log\pi(y_t|z_t^k)\Big] \
&amp;=-\mathbb{E}<em>{y_t\sim\pi(\cdot|z_t^k)}\Big[\nabla</em>{z_t^k}\log\pi(y_t|z_t^k)\cdot \log\pi(y_t|z_t^k)\Big]
\end{align*}
$$
​	Now, we proceed to compute the inner product $\langle \nabla_{z_t^k}H_t(z_t^k),z_t^{k+1} - z_t^k \rangle$. For notational simplicity, let  $\Delta z_t^k=z_t^{k+1}-z_t^k$.
$$
\begin{align*}
\langle \nabla_{z_t^k}H_t(z_t^k),\Delta z_t^k \rangle &amp;= -\langle\mathbb{E}<em>{y_t\sim\pi(\cdot|z_t^k)}\Big[\nabla</em>{z_t^k}\log\pi(y_t|z_t^k)\cdot \log\pi(y_t|z_t^k)\Big],\Delta z_t^k\rangle \
&amp;=-\mathbb{E}<em>{y_t\sim\pi(\cdot|z_t^k)}\Big[\log\pi(y_t|z_t^k)\langle \nabla</em>{z_t^k}\log\pi(y_t|z_t^k),\Delta z_t^k \rangle\Big] \
&amp;=-\mathbb{E}<em>{y_t\sim\pi(\cdot|z_t^k)}\Big[\log\pi(y_t|z_t^k)\sum</em>{j=1}^{|V|}\frac{\partial\log\pi(y_t|z_t^k)}{\partial z_{t,j}^k}\Delta z_{t,j}^k\Big]
\end{align*}
$$
Based on the derivative of the long-softmax function, we have:
$$
\begin{align*}
\sum_{j=1}^{|V|}\frac{\partial\log\pi(y_t|z_t^k)}{\partial z_{t,j}^k}\Delta z_{t,j}^k&amp;=\sum_{j=1}^{|V|}\textbf{1}{y_t=v_j}\Delta z_{t,j}^k-\pi(v_j|z_t^k)\Delta z_{t,j}^k\
&amp;=\Delta z_{t,y_t}^k-\mathbb{E}<em>{v\sim\pi(\cdot|z_t^k)}[\Delta z</em>{t,v}^k]
\end{align*}
$$
Therefore:
$$
\langle \nabla_{z_t^k}H_t(z_t^k),\Delta z_t^k \rangle=-\mathbb{E}<em>{y_t\sim\pi(\cdot|z_t^k)}\Big[\log\pi(y_t|z_t^k)\cdot\Big(\Delta z</em>{t,y_t}^k-\mathbb{E}<em>{v\sim\pi(\cdot|z_t^k)}[\Delta z</em>{t,v}^k]\Big)
$$
​	Let the random variables be $X=\log\pi(y_t|z_t^k)$ and $Y=\Delta z_{t,y_t}^k$. The expression above is precisely the negative covariance of the two random variables, i.e. :
$$
H_t(z_t^{k+1}) - H_t(z_t^k)\approx-\text{Cov}<em>{y_t\sim\pi(\cdot|z_t^k)}\Big(\log\pi(y_t|z_t^k),\Delta z</em>{t,y_t}^k\Big)
$$
    </div>
</div>
<p>​	[1] further shows that under natural policy gradient, the change in entropy is related to advantage as follows:
$$
H_t(z_t^{k+1}) - H_t(z_t^k)\approx-\eta\cdot \text{Cov}_{y_t\sim\pi(\cdot|z_t^k)}\Big(\log\pi(y_t|z_t^k),A^k(y_t,z_t^k)\Big)\quad(12)
$$
where $\eta$ is the learning rate and $A^k(y_t,z_t^{k})$ is the advantage of $y_t$ at the current state. In GRPO with binary rewards, positive samples have a positive advantage and negative samples have a negative one. This means that entropy decreases when updating on high-probability tokens from positive samples or low-probability tokens from negative samples.</p>
<p>​	Based on equation (12), [1] argues that update magnitude of high-covariance tokens should be limited, proposing <code>Clip-Cov</code> and <code>KL-Cov</code>. <code>Clip-Cov</code> primarily works by setting the gradients of identified high-covariance tokens to zero. <code>KL-Cov</code>, on the other hand, applies a KL-divergence constraint to identified tokens, preventing them from deviating too far from the original policy.</p>
<hr>
<h1 id="6-a-unified-perspective">6. A Unified Perspective<a hidden class="anchor" aria-hidden="true" href="#6-a-unified-perspective">#</a></h1>
<p>​	Fundamentally, the methods in [1], [3], [4], [5], and [6] all center on the entropy of high-entropy <code>forking tokens</code>. The approaches in [4], [5] and [6] mitigate the rapid decline in entropy by correcting the bias in GRPO against high-entropy(low-probability) tokens. In contrast, [1] alleviates rapid entropy decay by restricting the update magnitude of low-entropy(high-probability) tokens. Therefore, these approaches are theoretically compatible and could be combined, simultaneously increasing the update magnitude for high-entropy tokens while decreasing it for low-entropy ones.</p>
<p>​	[7] shifts the focus away from token-level entropy, proposing instead that the relative weight of negative samples should be increased. Interpreting this through the lens of [1]&rsquo;s framework, since negative samples generated by model are unlikely to have extremely low probabilities, optimizing on these relatively high-probability negative samples also helps to counteract entropy collapse.</p>
<hr>
<h1 id="references">References<a hidden class="anchor" aria-hidden="true" href="#references">#</a></h1>
<p>[1]. <a href="https://arxiv.org/pdf/2505.22617">The Entropy Mechanism of Reinforcement Learning for Reasoning Language Models</a></p>
<p>[2]. <a href="https://arxiv.org/pdf/2505.24864">ProRL: Prolonged Reinforcement Learning Expands Reasoning Boundaries in Large Language Models</a></p>
<p>[3]. <a href="https://arxiv.org/pdf/2506.01939">Beyond the 80/20 Rule: High-Entropy Minority Tokens Drive Effective Reinforcement Learning for LLM Reasoning</a></p>
<p>[4]. <a href="https://arxiv.org/pdf/2503.14476">DAPO: An Open-Source LLM Reinforcement Learning System at Scale</a></p>
<p>[5]. <a href="https://arxiv.org/pdf/2506.13585">MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention</a></p>
<p>[6]. <a href="https://arxiv.org/pdf/2506.14758">Reasoning with Exploration: An Entropy Perspective</a></p>
<p>[7]. <a href="https://arxiv.org/pdf/2506.01347">The Surprising Effectiveness of Negative Reinforcement in LLM Reasoning</a></p>
<p>[8]. <a href="https://arxiv.org/pdf/2506.17219">No Free Lunch: Rethinking Internal Feedback for LLM Reasoning</a></p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>


<ul class="share-buttons">
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Entropy Collapse and Mitigation Strategies on x"
            href="https://x.com/intent/tweet/?text=Entropy%20Collapse%20and%20Mitigation%20Strategies&amp;url=http%3a%2f%2flocalhost%3a1313%2fposts%2fentropy-collapse-and-mitigation-strategies%2f&amp;hashtags=">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M512 62.554 L 512 449.446 C 512 483.97 483.97 512 449.446 512 L 62.554 512 C 28.03 512 0 483.97 0 449.446 L 0 62.554 C 0 28.03 28.029 0 62.554 0 L 449.446 0 C 483.971 0 512 28.03 512 62.554 Z M 269.951 190.75 L 182.567 75.216 L 56 75.216 L 207.216 272.95 L 63.9 436.783 L 125.266 436.783 L 235.9 310.383 L 332.567 436.783 L 456 436.783 L 298.367 228.367 L 432.367 75.216 L 371.033 75.216 Z M 127.633 110 L 164.101 110 L 383.481 400.065 L 349.5 400.065 Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Entropy Collapse and Mitigation Strategies on linkedin"
            href="https://www.linkedin.com/shareArticle?mini=true&amp;url=http%3a%2f%2flocalhost%3a1313%2fposts%2fentropy-collapse-and-mitigation-strategies%2f&amp;title=Entropy%20Collapse%20and%20Mitigation%20Strategies&amp;summary=Entropy%20Collapse%20and%20Mitigation%20Strategies&amp;source=http%3a%2f%2flocalhost%3a1313%2fposts%2fentropy-collapse-and-mitigation-strategies%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Entropy Collapse and Mitigation Strategies on reddit"
            href="https://reddit.com/submit?url=http%3a%2f%2flocalhost%3a1313%2fposts%2fentropy-collapse-and-mitigation-strategies%2f&title=Entropy%20Collapse%20and%20Mitigation%20Strategies">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Entropy Collapse and Mitigation Strategies on facebook"
            href="https://facebook.com/sharer/sharer.php?u=http%3a%2f%2flocalhost%3a1313%2fposts%2fentropy-collapse-and-mitigation-strategies%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
            </svg>
        </a>
    </li>
</ul>

  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="http://localhost:1313/">Qiangwei Bai&#39;s Blog</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
